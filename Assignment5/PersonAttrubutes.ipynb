{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/roshantac/EIP4/blob/master/Assignment5/PersonAttrubutes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Gyq8CE4ug5BK"
   },
   "outputs": [],
   "source": [
    "# mount gdrive and unzip data\n",
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')\n",
    "!unzip -q \"/content/gdrive/My Drive/hvc_data.zip\"\n",
    "# look for `hvc_annotations.csv` file and `resized` dir\n",
    "%ls "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bYbNQzK6kj94"
   },
   "outputs": [],
   "source": [
    "%tensorflow_version 1.x\n",
    "\n",
    "import cv2\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from functools import partial\n",
    "from pathlib import Path \n",
    "from tqdm import tqdm\n",
    "\n",
    "from google.colab.patches import cv2_imshow\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from keras.applications.inception_resnet_v2 import InceptionResNetV2\n",
    "from keras.layers.core import Dropout\n",
    "from keras.layers.core import Flatten\n",
    "from keras.layers.core import Dense\n",
    "from keras.layers import Input\n",
    "from keras.models import Model\n",
    "from keras.optimizers import SGD\n",
    "from keras.preprocessing.image import ImageDataGenerator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vQkbSpLK4sTP"
   },
   "outputs": [],
   "source": [
    "# load annotations\n",
    "df = pd.read_csv(\"hvc_annotations.csv\")\n",
    "del df[\"filename\"] # remove unwanted column\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "202OJva345WA"
   },
   "outputs": [],
   "source": [
    "# one hot encoding of labels\n",
    "\n",
    "one_hot_df = pd.concat([\n",
    "    df[[\"image_path\"]],\n",
    "    pd.get_dummies(df.gender, prefix=\"gender\"),\n",
    "    pd.get_dummies(df.imagequality, prefix=\"imagequality\"),\n",
    "    pd.get_dummies(df.age, prefix=\"age\"),\n",
    "    pd.get_dummies(df.weight, prefix=\"weight\"),\n",
    "    pd.get_dummies(df.carryingbag, prefix=\"carryingbag\"),\n",
    "    pd.get_dummies(df.footwear, prefix=\"footwear\"),\n",
    "    pd.get_dummies(df.emotion, prefix=\"emotion\"),\n",
    "    pd.get_dummies(df.bodypose, prefix=\"bodypose\"),\n",
    "], axis = 1)\n",
    "one_hot_df.head().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0ll94zTv6w5i"
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "\n",
    "# Label columns per attribute\n",
    "_gender_cols_ = [col for col in one_hot_df.columns if col.startswith(\"gender\")]\n",
    "_imagequality_cols_ = [col for col in one_hot_df.columns if col.startswith(\"imagequality\")]\n",
    "_age_cols_ = [col for col in one_hot_df.columns if col.startswith(\"age\")]\n",
    "_weight_cols_ = [col for col in one_hot_df.columns if col.startswith(\"weight\")]\n",
    "_carryingbag_cols_ = [col for col in one_hot_df.columns if col.startswith(\"carryingbag\")]\n",
    "_footwear_cols_ = [col for col in one_hot_df.columns if col.startswith(\"footwear\")]\n",
    "_emotion_cols_ = [col for col in one_hot_df.columns if col.startswith(\"emotion\")]\n",
    "_bodypose_cols_ = [col for col in one_hot_df.columns if col.startswith(\"bodypose\")]\n",
    "\n",
    "class PersonDataGenerator(keras.utils.Sequence):\n",
    "    \"\"\"Ground truth data generator\"\"\"\n",
    "\n",
    "    \n",
    "    def __init__(self, df, batch_size=32, shuffle=True):\n",
    "        self.df = df\n",
    "        self.batch_size=batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.floor(self.df.shape[0] / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"fetch batched images and targets\"\"\"\n",
    "        batch_slice = slice(index * self.batch_size, (index + 1) * self.batch_size)\n",
    "        items = self.df.iloc[batch_slice]\n",
    "        image = np.stack([cv2.imread(item[\"image_path\"]) for _, item in items.iterrows()])\n",
    "        target = {\n",
    "            \"gender_output\": items[_gender_cols_].values,\n",
    "            \"image_quality_output\": items[_imagequality_cols_].values,\n",
    "            \"age_output\": items[_age_cols_].values,\n",
    "            \"weight_output\": items[_weight_cols_].values,\n",
    "            \"bag_output\": items[_carryingbag_cols_].values,\n",
    "            \"pose_output\": items[_bodypose_cols_].values,\n",
    "            \"footwear_output\": items[_footwear_cols_].values,\n",
    "            \"emotion_output\": items[_emotion_cols_].values,\n",
    "        }\n",
    "        return image, target\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        \"\"\"Updates indexes after each epoch\"\"\"\n",
    "        if self.shuffle == True:\n",
    "            self.df = self.df.sample(frac=1).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yVE8-OaZ8J5q"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_df, val_df = train_test_split(one_hot_df, test_size=0.15)\n",
    "train_df.shape, val_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "K5m15DLyF2ot"
   },
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oTiOi5tVBnhS"
   },
   "outputs": [],
   "source": [
    "# create train and validation data generators\n",
    "train_gen = PersonDataGenerator(train_df, batch_size=32)\n",
    "valid_gen = PersonDataGenerator(train_df, batch_size=64, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2pMDGat-Ghow"
   },
   "outputs": [],
   "source": [
    "# get number of output units from data\n",
    "images, targets = next(iter(train_gen))\n",
    "num_units = { k.split(\"_output\")[0]:v.shape[1] for k, v in targets.items()}\n",
    "num_units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZISqOzai4IvY"
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_random_eraser(p=0.5, s_l=0.02, s_h=0.4, r_1=0.3, r_2=1/0.3, v_l=0, v_h=255, pixel_level=False):\n",
    "    def eraser(input_img):\n",
    "        img_h, img_w, img_c = input_img.shape\n",
    "        p_1 = np.random.rand()\n",
    "\n",
    "        if p_1 > p:\n",
    "            return input_img\n",
    "\n",
    "        while True:\n",
    "            s = np.random.uniform(s_l, s_h) * img_h * img_w\n",
    "            r = np.random.uniform(r_1, r_2)\n",
    "            w = int(np.sqrt(s / r))\n",
    "            h = int(np.sqrt(s * r))\n",
    "            left = np.random.randint(0, img_w)\n",
    "            top = np.random.randint(0, img_h)\n",
    "\n",
    "            if left + w <= img_w and top + h <= img_h:\n",
    "                break\n",
    "\n",
    "        if pixel_level:\n",
    "            c = np.random.uniform(v_l, v_h, (h, w, img_c))\n",
    "        else:\n",
    "            c = np.random.uniform(v_l, v_h)\n",
    "\n",
    "        input_img[top:top + h, left:left + w, :] = c\n",
    "\n",
    "        return input_img\n",
    "\n",
    "    return eraser\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "03W8Pagg_Ppp"
   },
   "outputs": [],
   "source": [
    "backbone = InceptionResNetV2(include_top=False, weights=None, input_tensor=Input(shape=(224, 224, 3)), pooling=max)\n",
    "\n",
    "neck = backbone.output\n",
    "neck = Flatten(name=\"flatten\")(neck)\n",
    "neck = Dense(512, activation=\"relu\")(neck)\n",
    "\n",
    "\n",
    "def build_tower(in_layer):\n",
    "    neck = Dropout(0.2)(in_layer)\n",
    "    neck = Dense(128, activation=\"relu\")(neck)\n",
    "    neck = Dropout(0.3)(in_layer)\n",
    "    neck = Dense(128, activation=\"relu\")(neck)\n",
    "    return neck\n",
    "\n",
    "\n",
    "def build_head(name, in_layer):\n",
    "    return Dense(\n",
    "        num_units[name], activation=\"softmax\", name=f\"{name}_output\"\n",
    "    )(in_layer)\n",
    "\n",
    "# heads\n",
    "\n",
    "gender = build_head(\"gender\", build_tower(neck))\n",
    "image_quality = build_head(\"image_quality\", build_tower(neck))\n",
    "age = build_head(\"age\", build_tower(neck))\n",
    "weight = build_head(\"weight\", build_tower(neck))\n",
    "bag = build_head(\"bag\", build_tower(neck))\n",
    "footwear = build_head(\"footwear\", build_tower(neck))\n",
    "emotion = build_head(\"emotion\", build_tower(neck))\n",
    "pose = build_head(\"pose\", build_tower(neck))\n",
    "\n",
    "'''\n",
    "gender = build_head(\"gender\", \"softmax\",build_tower(neck))\n",
    "image_quality = build_head(\"image_quality\",\"sigmoid\", build_tower(neck))\n",
    "age = build_head(\"age\",\"sigmoid\", build_tower(neck))\n",
    "weight = build_head(\"weight\", \"sigmoid\",build_tower(neck))\n",
    "bag = build_head(\"bag\", \"sigmoid\" ,build_tower(neck))\n",
    "footwear = build_head(\"footwear\", \"sigmoid\",build_tower(neck))\n",
    "emotion = build_head(\"emotion\", \"sigmoid\",build_tower(neck))\n",
    "pose = build_head(\"pose\", \"sigmoid\",build_tower(neck))\n",
    "'''\n",
    "model = Model(\n",
    "    inputs=backbone.input, \n",
    "    outputs=[gender, image_quality, age, weight, bag, footwear, pose, emotion]\n",
    ")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BYr9F7C3aggk"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer=SGD(lr=0.001, momentum=0.9), loss=\"categorical_crossentropy\",  metrics=[\"accuracy\"])\n",
    "train_df.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tTQyEZlV0Yk5"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KsaxNIY94t7_"
   },
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "    # set input mean to 0 over the dataset\n",
    "    featurewise_center=False,\n",
    "    # set each sample mean to 0\n",
    "    samplewise_center=False,\n",
    "    # divide inputs by std of dataset\n",
    "    featurewise_std_normalization=False,\n",
    "    # divide each input by its std\n",
    "    samplewise_std_normalization=False,\n",
    "    # apply ZCA whitening\n",
    "    zca_whitening=False,\n",
    "    # epsilon for ZCA whitening\n",
    "    zca_epsilon=1e-06,\n",
    "    # randomly rotate images in the range (deg 0 to 180)\n",
    "    rotation_range=0,\n",
    "    # randomly shift images horizontally\n",
    "    width_shift_range=0.1,\n",
    "    # randomly shift images vertically\n",
    "    height_shift_range=0.1,\n",
    "    # set range for random shear\n",
    "    shear_range=0.,\n",
    "    # set range for random zoom\n",
    "    zoom_range=0.,\n",
    "    # set range for random channel shifts\n",
    "    channel_shift_range=0.,\n",
    "    # set mode for filling points outside the input boundaries\n",
    "    fill_mode='nearest',\n",
    "    # value used for fill_mode = \"constant\"\n",
    "    cval=0.,\n",
    "    # randomly flip images\n",
    "    horizontal_flip=True,\n",
    "    # randomly flip images\n",
    "    vertical_flip=False,\n",
    "    # set rescaling factor (applied before any other transformation)\n",
    "    rescale=None,\n",
    "    # set function that will be applied on each input\n",
    "    preprocessing_function=get_random_eraser(v_l=0, v_h=1, pixel_level=True),\n",
    "    # image data format, either \"channels_first\" or \"channels_last\"\n",
    "    data_format=None,\n",
    "    # fraction of images reserved for validation (strictly between 0 and 1)\n",
    "    validation_split=0.0)\n",
    "\n",
    "'''train_generator=datagen.flow_from_dataframe(dataframe=train_df, x_col='image_path', y_col=('gender_female', 'gender_male', 'imagequality_Average',\n",
    "       'imagequality_Bad', 'imagequality_Good', 'age_15-25', 'age_25-35',\n",
    "       'age_35-45', 'age_45-55', 'age_55+', 'weight_normal-healthy',\n",
    "       'weight_over-weight', 'weight_slightly-overweight',\n",
    "       'weight_underweight', 'carryingbag_Daily/Office/Work Bag',\n",
    "       'carryingbag_Grocery/Home/Plastic Bag', 'carryingbag_None',\n",
    "       'footwear_CantSee', 'footwear_Fancy', 'footwear_Normal',\n",
    "       'emotion_Angry/Serious', 'emotion_Happy', 'emotion_Neutral',\n",
    "       'emotion_Sad', 'bodypose_Back', 'bodypose_Front-Frontish',\n",
    "       'bodypose_Side'), target_size=(229, 229), color_mode='rgb',class_mode=\"categorical\", batch_size=32)'''\n",
    "# Compute quantities required for featurewise normalization\n",
    "# (std, mean, and principal components if ZCA whitening is applied).\n",
    "#datagen.fit(x_train)\n",
    "\n",
    "# Fit the model on the batches generated by datagen.flow().\n",
    "history = model.fit_generator(\n",
    "                    generator=train_gen,\n",
    "                    validation_data=valid_gen,\n",
    "                    use_multiprocessing=True,\n",
    "                    workers=25,\n",
    "                    epochs=100, \n",
    "                    verbose=1, \n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zI1hJb4qM6OH"
   },
   "outputs": [],
   "source": [
    "model"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "PersonAttrubutes(1).ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
